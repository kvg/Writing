\chapter{Discussion}
\label{ch:discussion}

\newthought{In this dissertation, we have demonstrated a graphical method} for the detection of \textit{de novo} mutations.  We have attempted to address six major concerns regarding detection of DNMs in genomes.  First, false-positive rates are hard to control in reference-based analyses, typically requiring myriad filters with dubious justifications.  Second, sensitivity is difficult to ascertain, as estimates would normally require an assessment of every base in the genome.  Without whole-genome assemblies of all samples (a prohibitively expensive prospect), such sensitivity measures cannot be produced.  Third, reference-based analyses cannot provide access to accessory regions of the genome - regions that are clinically interesting and often highly divergent between different members of the population.  Fourth, a new method should scale beyond the genomes of small parasites, all the way to sizes typical of mammalian genomes and beyond.  Fifth, in keeping with the need to scale beyond parasite genomes, it must also be capable of handling non-haploid data.  Finally, the method must be robust to low read coverage.  All six elements are addressed by the implementation and application of our graphical variant caller, designed to operate without the need for a reference sequence and on the novel kmers produced by the mutational processes that generate \textit{de novo} mutations.

\section{Reference versus graph}

\subsection{Indirect comparison: reference-based analysis}

Canonical reference sequences have been tremendously beneficial for analyses of genetic variation in sequenced samples.  Reference genomes are expensive and laborious to produce, requiring the use of sample preparation and sequencing strategies that do not scale to many samples (hundreds, if not thousands).  Aligning short, lower-quality reads from inexpensive second-generation sequencing technologies to a reference genome enables the comparison of many samples.  Nearly all general purpose variant-calling software packages are implemented as reference-based methods, reliant on pileups of short reads aligned to correct locations in the reference in order to discover a variant.

However, given the limitations of current technology, this procedure has a tacit reference bias.  Second-generation sequencing reads tend to be short ($75-150$ bp or so), and alignment algorithms require a significant amount of each read to align to the reference without error (mismatches, insertions, or deletions).  If the region in question has low diversity in the population, this protocol works very well.  If the sampled haplotypes deviate substantially from the reference haplotype, two types of error may occur.  First, reads may fail to map correctly, leading to false-positive calls.  Alternatively, reads may fail to map at all, leading to false negative calls.

Thus, one find themselves in an ironic position for the discovery of \textit{de novo} mutations: despite having data for the parental and child haplotypes, discovering mutations in the child with respect to its closest relatives first requires an \textit{indirect} analysis: a reference-mediated comparison, where the reference is usually unrelated to the samples in question.  If the reference haplotype is substantially different than the sampled haplotypes, one is likely to encounter mistakes that compromise the accuracy of the callset.

Aligning reads to a reference genome and calling variants is essentially taking a reference genome to be an initial hypothesis of the genome sequence, then refining that hypothesis with the variant calls.  However, the ability to refine that hypothesis is dependent on the properties of the data used to refine it.  With second-generation sequencing, our reads are too short to refine this initial hypothesis - the reference genome - when the true genome deviates substantially from it.  As we have seen in \textit{P. falciparum}, the initial hypothesis serves the core regions of the genome well, but the accessory regions very poorly, as these regions harbor antigenic genes and are thus under intense selection by immune systems.  In any species, regions under such diversifying selective pressure will typically be a struggle to treat in reference-based studies.

\subsection{Direct comparision: graph-based analysis}

Our work demonstrates an alternative approach: a hypothesis-free method based on \textit{de novo} assembly of genomes.  Constructing de Bruijn graphs for each member of a trio (mother, father, and child) is reasonably straightforward, and combining them into a multi-color de Bruijn graph facilitates their comparison.  Given that our samples are deeply covered (recall the Lander-Waterman statistics presented in \ref{tb:lw_cov}), we expect the entire genome to be represented in these graphs.  Thus, we remove the need for a canonical reference sequence to mediate the comparison, instead relying on the samples themselves for the variant discovery process.  This graphical approach is a more direct framework for DNM discovery as we are comparing the child's haplotypes to the haplotypes from which they are immediately derived, without the need to first compare them to an unrelated reference haplotype.

\section{Implementation notes}

\subsection{Merits of the graphical approach}

In implementing a graphical approach to DNM discovery, many merits to the approach over read alignment and contig alignment become apparent.  First, discovering where a novel kmer exists in a graph is very straightforward.  A graph can be thought of as a hashtable, in which case lookup time is $\mathcal{O}(1)$, but the entire graph needs to be stored in memory.  Our sorted-graph-on-disk implementation, wherein we perform a binary search over the sorted kmers in the graph, avoids the need for storing the entire graph in memory and has average lookup time of $\mathcal{O}(\log{}n)$, which is still very fast.  Additionally, as \textit{de novo} mutations are rare, the number of kmers in the graph that need be accessed is much smaller than the total number of kmers in the graph.  In contrast, it is less straightforward to search for novel kmers in the reads and contigs.  Without an index and without storing every read or contig in memory, one must resort to more primitive methods.  For example, one could find all reads and contigs with an $\mathcal{O}(nm)$ lookup: a linear traversal in which $n$ reads or contigs are examined, all $m$ kmers in the read are examined for presence in a novel kmer hash, and the read or contig is stored for later use if the kmer is present in the hash.

Smarter methods to identify all reads or contigs containing novel kmers certainly exist (e.g. aligning novel kmers to the reads or contigs themselves), but we quickly run into the second benefit of the graphical method over others: the de Bruijn graph imposes a restriction that kmers may only appear once in the data structure.  Thus, finding the kmer yields a single region of the graph to examine.  If implemented as a lookup on reads or contigs, we would discover many reads with many potential homes around the genome (some correct, some incorrect).  At best, each novel kmer could identify "active regions" around the genome - windows on the genome that may harbor DNMs.  However, other reads harboring the novel kmer and a mismatch later in the read sequence might align to a different region of the genome, yielding two active regions and no convenient way of combining them or eliminating regions from consideration.

Third, the novel kmers track progress in discovering DNMs in a way that only makes sense for the graphical method.  As we have shown, DNMs typically generate runs of novel kmers rather than isolated novel kmers.  These novel kmers span the variant in question.  In the ideal case where there are no sequencing errors, the reference-based approach could yield novel kmers that are all aligned to successive positions, easily revealing the narrow window in which the variant occurs.  However, in the non-ideal case, misaligned reads may not reveal the active region so precisely.

Fourth, true-but-polymorphic DNMs are much more easily identified in the graphical method.  As mentioned in the previous chapter, these mutations can arise in \textit{P. falciparum} during culture.  They are not false-positives per se, but depending on the researcher's question, they may or may not want to be considered as part of the DNM callset.  It is convenient to ignore such sites in the reference-based analysis of haploid samples; doing so focuses one's effort on monomorphic sites which are less likely to suffer from false-positives owing to the amount of evidence required to identify them.  If working with contigs rather than the graph, the assembler's attempt to linearize contigs by popping low-coverage branches of bubbles may erase evidence of a variant.  In diploid samples, such filters are hugely problematic.  Most DNMs would be heterozygous, and ignoring them is not an option.  With the reference-based analysis, the variant calling protocols must be different depending on whether one is working with haploid or diploid samples.  For our graphical approach, they are one and the same: if a novel kmer associates with a bubble motif, it is considered \textit{de novo}.  The genotype of the variant is not considered until after the variant is identified, and is not used as a filter. 

Finally, our random-access approach to graph traversal allows us to process genomes of any size without needing to load an entire graph into memory.  As \textit{de novo} variants are rare, the number of vertices we need to visit in the graph is far lower that the number of vertices in the graph.  This enables our method to scale to much larger (e.g. mammalian) genomes.  Analysis of the \textit{Pan troglodytes verus} pedigree in Chapter \ref{ch:chimp}, wherein each sample required $< 8$ hours of processing time, directly demonstrates this point.  Low coverage ($\approx 20x$) in the $F1$ generation of this dataset, along with the need to detect heterozygotes in diploid data, poses additional challenges to achieving callset specificity.  However, with additional filters designed to reexamine dirty data for evidence that refutes DNM calls, we were able to produce a DNM callset comparable (if not more sensitive and specific) than the reference-based analysis.

\subsection{Challenges of the graphical approach}

There are several challenges that must be met in developing a graphical approach for DNM discovery.  First and foremost, very little shared infrastructure around genome graphs exists.  Reference-based analyses benefit from standardized file formats (FASTA, BAM, VCF, BED, GFF, etc.).  The GATK, Platypus, Samtools, Picard, SOAPsnp, and many others, provide dozens of data manipulation tools for next-generation sequencing data.  The htslib C library and the htsjdk Java library enable rapid and random access to compressed sequence data formats.  While many tools for \textit{de novo} assembly exist, there is no standard file format for describing the underlying graph data structure, and there is no readily available library for interrogating the file formats that do exist.  Assemblers are generally written such that the user is only ever intended to see the final contig output; the particulars of the graph are private to the software, not for user consumption.

Very little software exists for visualizing graphs so that an analyst can inspect various aspects of a graph to convince themselves of the credibility of a call or callset.  Reference-based methods benefit from the existence of several visualization tools, all free for public use (IGV, Tablet\cite{Milne:2013gf}).  Visualization tools for genome graphs are few and far between, require very specific file formats, can be incredibly demanding of memory resources, do not understand multiple samples, and typically only display graphs at the contig level, rather than the kmer level\cite{Wick:2015kx,Weisenfeld:2014dt}.  General-purpose graph visualization algorithms can be used to visualize genome graphs at the kmer level, but result in static layouts that attempt to maximize some arbitrary aesthetic properties (e.g. preserve node hierarchy, minimize edge crossings, avoid sharp bends in edges, keep edges short, maximize symmetry, etc.\cite{Gansner:1993kb}).

Graphs are far more susceptible to data contamination issues than alignment-based methods.  With alignment, the reference genome acts as a sort of sieve, retaining only reads that are similar in sequence to some region of the reference.  The same failing that prevents us from accessing the subtelomeric regions of the genome protects us from processing sequence contamination as if it were part of the genome.  In the graphical method, this protection does not exist; all kmers are added to the graph, no matter the source.  If a contaminant happens to share a single low-complexity kmer with the samples in question, the genomes become linked through the graph.  As we have seen, tremendous effort must be applied to mitigate issues involving data contamination.

Localizing variants can be problematic.  Although our method does not rely on the presence of a reference genome, for post-calling analysis, it is of course desirable to place the variant in its genomic context.  Determining this location can be tricky.  A lookup table for every kmer in the reference genome and all of its possible homes is helpful, but often a kmer ($47$ bp in our case) is still rather short and aligns ambiguously.  Instead of aligning single kmers, we can align the parental branch spanning a bubble rather than the child's branch.  This reduces the amount of variation that the aligner needs to contend with.  However, errors or other variants nearby can prematurely truncate the contig, and the truncated contigs may not align as readily.  Thus, the graphical approach can sometimes leave us with variants whose location is unclear.

Determining the haplotype background upon which a variant has occurred is not always straightforward.  The background for a variant situated in a genomic region where one parent is exceedingly different than the other's is easy to determine.  For a variant that occurs in a region that is perfectly homologous between the two parents, inspection of the graph is less revealing.  For a variant that occurs near another variant (perhaps a polymorphic variant), the event may appear on one background when it actually occurs on the other.  In theory, one should be able to walk the graph in either direction of a DNM until one sees inherited variation that tags one background or the other.  In practice, however, the ability to traverse past errors and repetitive regions is limited with such short reads, and its unlikely that one can establish enough genomic context to determine the background accurately and consistently.

\subsection{Addressing the implementation challenges}

On the issue of reusable software, we have provided tools for manipulating graphs and an object-oriented API for accessing the contents of the Cortex binary graph format so that other developers can do the same.  Our toolkit, INDIANA, is a framework for graph manipulation, graph-based variant calling, and visualization.  The framework is written in Java, ensuring that it can be run on any platform.  Embedded within it is our \texttt{CortexGraph} API is capable of iterating through the graph one record at a time, or (if the graph is sorted), providing random access to the records within the file.  We employ memory-mapped access to the file contents using the Java NIO (new I/O) API to achieve high-performance reads and writes.  The library can be compiled as a single Java archive (.jar) file so that it can be included some of the most popular Java-based toolkits for manipulating sequence data, including the GATK, Picard, and IGV.

To address the problem of visualization, we wrote \texttt{VisualCortex}.  \texttt{VisualCortex} is a lightweight web-based visualizer for Cortex graphs, built on top of our aforementioned Cortex API and the force-directed graph layout engine provided by the d3.js JavaScript visualization library\cite{Bostock:2011da}.  VisualCortex makes use of the JDK's built-in \texttt{HttpServer} object to expose a RESTful API for accessing graph data from a web browser, allowing us to fetch and display data without the need for page reloads after every request.  Our client provides a search interface for a user to specify a single kmer in the graph, which the server uses to fetch the local subgraph and associated metadata.  The traversals applied to the child and parent colors are precisely the same as used during variant calling, so the server can be used to evaluate the caller's decision-making process on traversing or ignoring branches in the graph.  Vertices and edges are drawn as scalable vector graphics (SVG) elements for later lossless export to PDF.  Child edges are drawn as straight lines, while parental edges are drawn as curves with opposite bends so that all three edges can be seen connecting two adjacent vertices simultaneously.  Edges involved in a variant call are drawn with thicker lines so as to be more visually apparent.  Vertices are color-coded to reveal novelty (red) or lack thereof (grey), and shape-coded to reveal contaminants (crosses) or normal data (circles).  Vertices can be repositioned by drag-and-drop gestures, permitting the user to choose a more aesthetically pleasing layout and/or one that calls attention to particular details of the graph.  Low-level graph data (i.e. the kmer sequence, coverage, and edge data from the underlying Cortex graph, and the kmer's location in the parental reference genomes if available) are displayed on vertex mouse-over events.  To enable exploration, double-clicking a vertex triggers an additional traversal which continues graph navigation for a short distance ($50$ vertices, or more than one junction, or until there are no more edges to explore - whichever comes first).  In theory, because our implementation follows the model-view-controller (MVC) design paradigm wherein the external representation of information is wholly separate from the internal representation, any graphical file format could be accomodated so long as it implemented the CortexGraph API.

Our decontamination procedure also makes use of our customizable traversal engine to find and mark likely contaminants so they can be ignored by the caller downstream.  We note that our procedure is implemented on the graph rather than the reads, despite it theoretically being easier to remove contaminating reads before graph construction than to remove kmers from the graph after the fact.  Our more complicated procedure is motivated by the pathological case that in two overlapping reads from a contaminant, only the first may have a kmer in the BLAST database.  If we had only removed reads with possible contaminants, the overlapping reads would remain in the graph and would still have a number of novel kmers that confuses our analysis.  Furthermore, removing the first read with contaminating kmers would discard the signal later needed to remove the second.

To map variants back to a canonical home in the genome, we employ multiple methods.  We keep an index for each parental genome specifying the location(s) of every kmer in the genome (a sorted list of a kmer and its location, with multiple records for the same kmer if it has multiple homes, all conveniently implemented via the Java-based MapDB library for on-disk storage of typical \texttt{java.util} collections\footnote{\url{http://www.mapdb.org/}}).  We've also implemented access to two external aligners, BWA and LASTZ, both of which return records in SAM format and latter of which returns better results when aligning long contigs.  The index allows us to quickly determine if a contig found in the child might be chimeric, while the external aligners are robust to sequencing errors and allow us to place variants in their genomic context.

Finally, unable to rely on graph traversals to consistently reveal the haplotypic background of an event, we instead used haplotype transmission tracks generated by the reference-based method.  These tracks are produced by simply calling variants in mother, father, and child along the reference sequence (ignoring the masked regions from the MalariaGen project), and for variants not shared by both parents, determining the parent of origin.  Successive variants with the same parent of origin are merged into large intervals and added to an interval tree, a tree data structure that allows one to find intervals contained by or overlapping with other intervals in $\mathcal{O}(\log{}n + m)$ time.  If a background determination cannot be made using the graph traversal, we use the localization procedure discussed above to determine a locus in the reference sequence and an interval tree lookup to determine the background.

\subsection{Novel kmers and their effect on design considerations}

The novel kmers concept is important to the design of our calling software.  In our implementation, they serve three functions: as beacons (where to look), as signposts (which direction to walk), and as mile markers (how much further to go).

As beacons, novel kmers indicate regions in the graph to examine for DNM activity, allowing us to process only the relevant parts of the graph rather than exploring every possible location.  Our code is written to iterate through the set of novel kmers and attempt to call a variant at each one.  We recognize that not all novel kmers tag \textit{unique} variants; many are found adjacent to one another as part of the same mutation.  Thus, we keep track of every novel kmer we jump to and every novel kmer we see during a variant-calling operation so as not to perform redundant work.  Because of this, any future improvements pursuing parallelization would have to be carefully considered.  Simply parallelizing the variant-calling operations by splitting up the list of novel kmers among processing threads would lead to a lot of redundant effort.  Instead, a better strategy might be to identify novel contigs associated with novel kmers, then parallelize on the contigs.

As signposts, novel kmers allow us to continue graph traversals that would otherwise be truncated due to an inability to navigate junctions at errors.  Here it is important to understand that we have made two explicit assumptions: adjacent novel kmers belong to the same variant, and \textit{de novo} mutations are rare.  The former assumption allows us to choose a reasonable branch when we encounter a junction with novel kmers on one output and not the other.  The latter assumption allows us to act on the first assumption without fear that we will accidentally traverse to an irrelevant region of the genome and output a nonsensical variant call.

Finally, as mile markers, novel kmers inform us as to how much of the graph remains to be explored.  While many other variant callers certainly have progress meters, they are typically measuring how many bases remain in the genome to be processed.  Our accounting is not simply a matter of tracking progress, but rather about reaching full sensitivity.  Novel kmers do not simply tell us where to look, they tell us when to stop looking.

There is an argument to be made that the amount of effort we expend to eliminate errors and contamination from the initial list of novel kmers is wasted.  After all, we have not observed traversals seeded from erroneous novel kmers resulting in false positive bubble-motif events, and mistaken linear-motif events are rare.  Rather, the false positives are more often unclassifiable events.  If there is little danger in flooding our callset with errors by increasing the list of novel kmers that we inspect, why try so hard to narrow that list?  Our answer is that neglecting to filter the novel kmer list would compromise its usefulness as a sensitivity measure.  Removing errors and contaminating kmers effectively provides an explanation for how those kmers came to be part of our dataset in the first place.  Had we not removed them, we might not have necessarily compromised the integrity of the final callset, but we would almost certainly be left wondering how so many unclassified events had come to be part of our data.

\section{Further improvements}

\subsection{Towards a probabilistic treatment of assembly graphs}

One interesting aspect of our approach is that it is the beginning of a probabilistic treatment of assembly graphs.  Assembly graphs are generally treated in a very binary way: a kmer is either an error or not; a contig either belongs in the graph or doesn't; navigation proceeds until it hits a junction, and makes no effort to traverse the boundary.  In contrast, we make a number of decisions that subvert this behavior.  Although we rely on McCortex and its error-cleaning algorithm for the actual construction of the de Bruijn graph data structure, we have implemented our code in such a way to allow for a kmer to be retreived from the dirty graph if it allows us to complete a traversal that otherwise could not occur.  In doing so, we recognize the fact that no error cleaning algorithm can ever be completely correct.  This is akin to having a quality score attached to each kmer (albeit one without much dynamic range).  Although many kmers might deserve a low quality score indicating their likelihood of being an error, some kmers clearly represent real data despite appearing erroneous at first glance.  We've implemented our system such that a dirty kmer can be incorporated into the traversal if a clean kmer is absent, but one could imagine a more probablistic treatment.

In fact, the traversal algorithm is already set up to permit a probabilistic treatment.  Dijkstra's shortest path algorithm attempts to find the shortest weight path, not necessarily the shortest number of kmers.  If all the edge weights are identical, these are one and the same.  However, weights could be easily incorporated into the algorithm, with higher weights indicating a lower quality.  If multiple paths from source to destination exist, the path with fewer dirty kmers will likely be chosen.  However, if the path utilizing the dirty kmers is the only path that can be found, they'll be used.  Although we intended to implement this functionality in our own code, we had difficulty finding clear examples in simulation where such traversal rules proved critical to recovering variants.  Thus we simply stuck with the ad-hoc incorporation of dirty kmers when necessary.  In general, the mutation rate of \textit{P. falciparum} appears to be too low and the read coverage too high to make adequate use of this feature.  In lower coverage data and/or data with a higher mutation rate, it may very well be a useful feature.

\subsection{Removing graphical tangles}

As we have seen, filters on the graph are necessary to remove artifactual novel kmers.  One graphical motif that we did not develop a filter for is the so-called "tangle", a bundle of low-complexity kmers with promiscuous connectivity.  Novel kmers likely arise in these regions as the result of recurrent sequencing errors promoted by low complxity sequence.  We speculate that these tangles can be treated as a "community detection" problem.  Formally, community detection aims to partition graphs such that each subset has many more edges connecting members within groups than between groups\cite{2010PhR...486...75F}.  A popular algorithm that attempts to optimize a modularity score (a scalar measure of intra-community vs inter-community links) could likely be applied\cite{Blondel:2008do}.

\subsection{Detecting inversions}

Our work did not reveal any inversions in the \textit{P. falciparum} or chimpanzee datasets.  However, we note that our detection apparatus is currently only set up for small events (substantially shorter than a read length) that look no different than typical bubble-motif events.  This was a conscious decision, as there is   Large inversions (substantially longer than a read length) will not necessarily exhibit this pattern.  Instead, we would expect to see novel kmers at the breakpoints, and inherited kmers between them with edges running in opposing directions between child and parent along the inverted segment\cite{Lemaitre:2014io}.  Detecting inversions is a desirable future feature of our graphical variant caller.

\subsection{Other miscellaneous improvements}

Other than being used during graph construction, cleaning, and novel kmer filtering, coverage is not inspected in the execution of our variant-calling process.  While we are able to detect insertions and deletions, we have made no attempt to detect copy number changes.  Coverage across the parental versus alternate branch can inform on this point.  In addition, potential recurrent sequencing artifacts in diploid data could be removed by requiring roughly equal coverage on each branch (though we note that, in theory, McCortex's graph cleaning algorithm should already make similar considerations).

%\section{Application to \textit{Plasmodium falciparum}}

%\section{Application to \textit{Pan troglodytes verus}}

\section{Conclusions}

We have shown our graphical variant calling method works well with simulated data (both idealized and realistic), real data from small genomes, and real data from large genomes.  Our approach enables datasets to be processed in a reference-free manner, eliminating the inherent reference bias of such approaches.  We thus have tremendous flexibility in how we search for \textit{de novo} mutations in terms of the types of variants we search for, dataset size, optional reference genome availability, and reference genome quality.  With the advent of third-generation sequencing, our graphical variant calling method should enable the study of genome biology in an era where long-read sequencing platforms are used to generate a draft-quality assembly for use as merely a coordinate system, while cheaper short-read sequencing platforms are used for refinements of the genome.
